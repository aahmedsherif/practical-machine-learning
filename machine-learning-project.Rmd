---
title: "Machine-Learning-Project"
author: "Ahmed Sherif"
date: "September 25, 2018"
output: html_document
---
 
 
## Background
### Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

 
### In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 


## Data Processing
### This section will download the data from the web if it is not already exist in the working directory
```{r echo=TRUE, cache=TRUE}




destfile <- "./pml-training.csv"
fileURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"

# if the data file is not exist then download it
if(!file.exists(destfile)){
   download.file(fileURL, destfile=destfile, method="auto") 
}

destfile <- "./pml-testing.csv"
fileURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

# if the data file is not exist then download it
if(!file.exists(destfile)){
   download.file(fileURL, destfile=destfile, method="auto") 
}

# read the data 
training <- read.csv("./pml-training.csv", header = TRUE,  na.strings=c("NA","#DIV/0!",""))
testing <- read.csv("./pml-testing.csv", header = TRUE,  na.strings=c("NA","#DIV/0!",""))

 
dim(training)
dim(testing)

```
 

### This section will remove the first seven columns from the data sets beacause they are irrelevant to the experiment 

```{r pressure, echo=TRUE}

set.seed(33833)
 
# remove first 7 columns, they don't affect the analysis
training <- training[, -c(1:7)]
testing <- testing[, -c(1:7)]
 
 
```


### This section fills the missing values in the datasets, the missing value will be replaced by  the mean of the column in case the column type is not a factor, otherwise it will be replaced by the most frequent level of the factor
```{r echo=TRUE, cache=TRUE}

fill_missing_values <- function(data){
  # for each column
  for(i in 1:ncol(data)){
  
      if(!is.factor(data[,i])){
      
        # if the column is not a factor then
        # replace NA values with the mean of the column or 0 if the column contains only NA values
        mean <- mean(data[,i], na.rm=TRUE)
        if(!is.na(mean))
        {
          data[is.na(data[,i]),i] <- mean
        }
        else
        {
          data[is.na(data[,i]),i] <- 0
        }
      
      }
      else
      {
        # if the column is a factor then
        # replace NA values with the most frequest Level in the column
        tt <- table(data[,i]) 
        data[is.na(data[,i]),i] <-  names(tt[which.max(tt)])
      }
    
  }
  return(data)
}

# replace NA with mean of columns
training <- fill_missing_values(training)
testing <- fill_missing_values(testing)
 
```


### This section splits the Training data into two sets for validation purposes
```{r echo=TRUE, cache=TRUE}

library(caret)

# split the training data for validation
inTrain = createDataPartition(training$classe, p = 3/4)[[1]]
myTraining = training[ inTrain,]
myValidation = training[-inTrain,]

```


### This section creates a model of type "Random Forest" starting with 500 trees
```{r echo=TRUE, cache=TRUE}
library(randomForest)   

# fit a random forest mod to our training set
mod <- randomForest(classe ~ ., data = myTraining, na.action = na.exclude, ntree=500) 
pred <- predict(mod, myValidation)

# accuracy
confusionMatrix(pred, myValidation$classe)$overall[1]

# plot the mod
plot(mod)

```


### Based on the previous plot, we found that we can reduce the number of trees while having the same magnitude of error, so we will fit a gain with 100 trees
```{r echo=TRUE, cache=TRUE}
library(randomForest)   

# reduce the number of trees 
mod <- randomForest(classe ~ ., data = myTraining, na.action = na.exclude, ntree=100)
pred <- predict(mod, myValidation)

# accuracy
pred_accuracy <- confusionMatrix(pred, myValidation$classe)$overall[1] 
pred_accuracy

```


### In this section, we will remove the features that have variance near zero and we will fit a model again, then we will compare the accuracies 
```{r , echo=TRUE}
library(caret)

# remove the columns with variance near to zero
near.zero.var.cols <- names(training)[nearZeroVar(training)] 
length(near.zero.var.cols)


```


### We found that the number of near-zero-variance columns is `r length(near.zero.var.cols)`, we will fit again without these columns

```{r , echo=TRUE}

library(randomForest)   
library(caret)

# remove the  near-zero-variance columns
training <- training [,!(colnames(training) %in% near.zero.var.cols)]
testing <- testing [,!(colnames(testing) %in% near.zero.var.cols)]

# split the training data
inTrain = createDataPartition(training$classe, p = 3/4)[[1]]
myTraining = training[ inTrain,]
myValidation = training[-inTrain,]

# fit a random forest mod to our training set
mod2 <- randomForest(classe ~ ., data = myTraining, na.action = na.exclude, ntree=100)
pred2 <- predict(mod2, myValidation)

# accuracy
confusionMatrix(pred2, myValidation$classe)
pred2_accuracy <- confusionMatrix(pred2, myValidation$classe)$overall[1]
pred2_accuracy
 
```

## Conclusion
### The accuracy while we use all the columns is `r pred_accuracy`, and the accuracy when we remove the near-zero-variance columns is `r pred2_accuracy`, We can find the accuracy is not affected greatly after removing the near-zero-variance columns, so we will choose that last mod to predict the Testing data, because this mod improves the performance

```{r , echo=TRUE}

pred_testing <- predict(mod2, testing)
pred_testing
```



